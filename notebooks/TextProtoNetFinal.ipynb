{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "from fastprogress import progress_bar\n",
    "import pickle\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(sampled_character_folders, labels, texts, nb_samples=None, shuffle=False):\n",
    "    if nb_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, nb_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    texts_labels = [(i, text_vectors[idx]) for i in range(len(sampled_character_folders)) for idx in sampler(list(np.where(np.array(labels) == i)[0]))]\n",
    "    if shuffle:\n",
    "        random.shuffle(texts_labels)\n",
    "    return texts_labels\n",
    "    \n",
    "class TextGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of text.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of classes for classification (K-way)\n",
    "            num_samples_per_class: num samples to generate per class in one batch\n",
    "            num_meta_test_classes: Number of classes for classification (K-way) at meta-test time\n",
    "            num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
    "            batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "        self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
    "        self.num_meta_test_classes = num_meta_test_classes\n",
    "        self.dim_input = 768\n",
    "        self.dim_output = 32\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df['target'].tolist()\n",
    "        #self.nlp = spacy.load('en_trf_bertbaseuncased_lg')\n",
    "        class_list = np.unique(np.array(df['target'].tolist()))\n",
    "        random.seed(1)\n",
    "        random.shuffle(class_list)\n",
    "        num_val = 6\n",
    "        num_train = 8\n",
    "        self.metatrain_character_folders = class_list[: num_train]\n",
    "        self.metaval_character_folders = class_list[num_train:num_train + num_val]\n",
    "        self.metatest_character_folders = class_list[num_train + num_val:]\n",
    "    def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False):\n",
    "        \"\"\"\n",
    "        Samples a batch for training, validation, or testing\n",
    "        Args:\n",
    "            batch_type: meta_train/meta_val/meta_test\n",
    "            shuffle: randomly shuffle classes or not\n",
    "            swap: swap number of classes (N) and number of samples per class (K) or not\n",
    "        Returns:\n",
    "            A a tuple of (1) Image batch and (2) Label batch where\n",
    "            image batch has shape [B, N, K, 768] and label batch has shape [B, N, K, N] if swap\n",
    "            where B is batch size, K is number of samples per class, N is number of classes\n",
    "        \"\"\"\n",
    "        if batch_type == \"meta_train\":\n",
    "            text_classes = self.metatrain_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        elif batch_type == \"meta_val\":\n",
    "            text_classes = self.metaval_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        else:\n",
    "            text_classes = self.metatest_character_folders\n",
    "            num_classes = self.num_meta_test_classes\n",
    "            num_samples_per_class = self.num_meta_test_samples_per_class\n",
    "        all_text_batches, all_label_batches = [], []\n",
    "        for i in range(batch_size):\n",
    "            sampled_character_folders = random.sample(list(text_classes), num_classes)\n",
    "            labels_and_texts = get_texts(sampled_character_folders, self.labels, self.texts, nb_samples=num_samples_per_class, shuffle=False)\n",
    "            labels = [li[0] for li in labels_and_texts]\n",
    "            texts_ = [li[1] for li in labels_and_texts]\n",
    "            texts = np.stack(texts_)\n",
    "            labels = np.array(labels)\n",
    "            labels = np.reshape(labels, (num_classes, num_samples_per_class))\n",
    "            labels = np.eye(num_classes)[labels]\n",
    "            texts = np.reshape(texts, (num_classes, num_samples_per_class, -1))\n",
    "            batch = np.concatenate([labels, texts], 2)\n",
    "            if shuffle:\n",
    "                for p in range(num_samples_per_class):\n",
    "                    np.random.shuffle(batch[:, p])\n",
    "            labels = batch[:, :, :num_classes]\n",
    "            texts = batch[:, :, num_classes:]\n",
    "            if swap:\n",
    "                labels = np.swapaxes(labels, 0, 1)\n",
    "                texts = np.swapaxes(texts, 0, 1)\n",
    "            all_text_batches.append(texts)\n",
    "            all_label_batches.append(labels)\n",
    "        all_text_batches = np.stack(all_text_batches)\n",
    "        all_label_batches = np.stack(all_label_batches)\n",
    "        return all_text_batches, all_label_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mini newsgroup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectors = pickle.load(open('../data/mini_newsgroup_vectors.pkl','rb'))\n",
    "mini_df = pickle.load(open('../data/mini_newsgroup_data.pkl','rb'))\n",
    "#mini_df, text_vectors = get_mini_dataset(samples_per_class = 30, embebeddings = 'BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete protonet pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNetText(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, proto_dim):\n",
    "        super(ProtoNetText, self).__init__()\n",
    "        self.embed_size = embedding_size\n",
    "        self.proto_dim = proto_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.l1 = torch.nn.Linear(self.embed_size, self.hidden_size)\n",
    "        self.rep_block =torch.nn.Sequential(*[torch.nn.BatchNorm1d(hidden_size), torch.nn.Linear(self.hidden_size, self.hidden_size)])\n",
    "        self.final = torch.nn.Linear(self.hidden_size, self.proto_dim)\n",
    "    def forward(self, x):\n",
    "        return self.final(self.rep_block(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries\n",
    "class ProtoLoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes, num_support, num_queries, ndim):\n",
    "        super(ProtoLoss,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_support = num_support\n",
    "        self.num_queries = num_queries\n",
    "        self.ndim = ndim\n",
    "    \n",
    "    def euclidean_distance(self, a, b):\n",
    "        N, D = a.shape[0], a.shape[1]\n",
    "        M = b.shape[0]\n",
    "        a = torch.repeat_interleave(a.unsqueeze(1), repeats = M, dim = 1)\n",
    "        b = torch.repeat_interleave(b.unsqueeze(0), repeats = N, dim = 0)\n",
    "        return 1.*torch.sum(torch.pow((a-b), 2),2)\n",
    "        \n",
    "    def forward(self, x, q, labels_onehot):\n",
    "        protox = torch.mean(1.*x.reshape([self.num_classes,self.num_support,self.ndim]),1)\n",
    "        dists = self.euclidean_distance(protox, q)\n",
    "        logpy = torch.log_softmax(-1.*dists,0).transpose(1,0).view(self.num_classes,self.num_queries,self.num_classes)\n",
    "        ce_loss = -1. * torch.mean(torch.mean(logpy * labels_onehot.float(),1))\n",
    "        accuracy = torch.mean((torch.argmax(labels_onehot.float(),-1).float() == torch.argmax(logpy,-1).float()).float())\n",
    "        return ce_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 5\n",
    "k_shot = 5\n",
    "proto_dim = 32\n",
    "n_query = 2\n",
    "n_meta_test_way = 5\n",
    "k_meta_test_shot = 5\n",
    "n_meta_test_query = 2\n",
    "num_epochs = 20\n",
    "num_episodes = 200\n",
    "hidden_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 768\n",
    "model_text = ProtoNetText(embed_size, hidden_dim, proto_dim)\n",
    "optimizer_text = torch.optim.Adam(model_text.parameters(), lr=1e-4)\n",
    "criterion = ProtoLoss(n_way, k_shot, n_query, proto_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator_ = TextGenerator(mini_df, n_way, k_shot+n_query, n_meta_test_way, k_meta_test_shot+n_meta_test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents(x,y, embed_size, n_way, n_query, k_shot):\n",
    "    x_support, x_query = x[:,:,:k_shot,:], x[:,:,k_shot:,:]\n",
    "    y_support, y_query = y[:,:,:k_shot,:], y[:,:,k_shot:,:]\n",
    "    labels_onehot = y_query.reshape(n_way, n_query, n_way)\n",
    "    support_input_t = torch.Tensor(x_support).view(-1, embed_size)\n",
    "    query_input_t = torch.Tensor(x_query).view(-1, embed_size)\n",
    "    return support_input_t, query_input_t, labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents_new(x,y, embed_size, n_way, n_query, k_shot):\n",
    "    lookup_dict = {i:np.where(y.reshape(-1,n_way)[:,i] == 1.)[0] for i in range(n_way)}\n",
    "    lookup_list = np.ravel([lookup_dict[i] for i in range(n_way)])\n",
    "    ### \n",
    "    x_shuffle = x.reshape(-1, embed_size)[lookup_list].reshape(1, n_way, n_query+k_shot, embed_size)\n",
    "    y_shuffle = y.reshape(-1, n_way)[lookup_list].reshape(1, n_way, n_query+k_shot, n_way)\n",
    "    ###\n",
    "    x_support, x_query = x_shuffle[:,:,:k_shot,:], x_shuffle[:,:,k_shot:,:]\n",
    "    y_support, y_query = y_shuffle[:,:,:k_shot,:], y_shuffle[:,:,k_shot:,:]\n",
    "    labels_onehot = y_query.reshape(n_way, n_query, n_way)\n",
    "    support_input_t = torch.Tensor(x_support).view(-1, embed_size)\n",
    "    query_input_t = torch.Tensor(x_query).view(-1, embed_size)\n",
    "    return support_input_t, query_input_t, labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoc 0/40 Episode 0/200, Validation Accuracy: 0.4, Validation Loss: 0.478\n",
      "Epoc 0/40 Episode 50/200, Validation Accuracy: 0.5, Validation Loss: 0.261\n",
      "Epoc 0/40 Episode 100/200, Validation Accuracy: 0.6, Validation Loss: 0.188\n",
      "Epoc 0/40 Episode 150/200, Validation Accuracy: 0.5, Validation Loss: 0.183\n",
      "Epoch: 1\n",
      "Epoc 1/40 Episode 0/200, Validation Accuracy: 0.7, Validation Loss: 0.155\n",
      "Epoc 1/40 Episode 50/200, Validation Accuracy: 0.8, Validation Loss: 0.142\n",
      "Epoc 1/40 Episode 100/200, Validation Accuracy: 0.6, Validation Loss: 0.165\n",
      "Epoc 1/40 Episode 150/200, Validation Accuracy: 0.3, Validation Loss: 0.289\n",
      "Epoch: 2\n",
      "Epoc 2/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.203\n",
      "Epoc 2/40 Episode 50/200, Validation Accuracy: 0.7, Validation Loss: 0.227\n",
      "Epoc 2/40 Episode 100/200, Validation Accuracy: 0.8, Validation Loss: 0.171\n",
      "Epoc 2/40 Episode 150/200, Validation Accuracy: 0.7, Validation Loss: 0.132\n",
      "Epoch: 3\n",
      "Epoc 3/40 Episode 0/200, Validation Accuracy: 0.6, Validation Loss: 0.281\n",
      "Epoc 3/40 Episode 50/200, Validation Accuracy: 0.3, Validation Loss: 0.305\n",
      "Epoc 3/40 Episode 100/200, Validation Accuracy: 0.3, Validation Loss: 0.306\n",
      "Epoc 3/40 Episode 150/200, Validation Accuracy: 0.9, Validation Loss: 0.072\n",
      "Epoch: 4\n",
      "Epoc 4/40 Episode 0/200, Validation Accuracy: 1.0, Validation Loss: 0.079\n",
      "Epoc 4/40 Episode 50/200, Validation Accuracy: 0.8, Validation Loss: 0.227\n",
      "Epoc 4/40 Episode 100/200, Validation Accuracy: 0.9, Validation Loss: 0.22\n",
      "Epoc 4/40 Episode 150/200, Validation Accuracy: 0.9, Validation Loss: 0.047\n",
      "Epoch: 5\n",
      "Epoc 5/40 Episode 0/200, Validation Accuracy: 0.8, Validation Loss: 0.198\n",
      "Epoc 5/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.067\n",
      "Epoc 5/40 Episode 100/200, Validation Accuracy: 0.7, Validation Loss: 0.085\n",
      "Epoc 5/40 Episode 150/200, Validation Accuracy: 0.3, Validation Loss: 0.298\n",
      "Epoch: 6\n",
      "Epoc 6/40 Episode 0/200, Validation Accuracy: 1.0, Validation Loss: 0.047\n",
      "Epoc 6/40 Episode 50/200, Validation Accuracy: 0.8, Validation Loss: 0.106\n",
      "Epoc 6/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.131\n",
      "Epoc 6/40 Episode 150/200, Validation Accuracy: 0.3, Validation Loss: 0.423\n",
      "Epoch: 7\n",
      "Epoc 7/40 Episode 0/200, Validation Accuracy: 0.8, Validation Loss: 0.171\n",
      "Epoc 7/40 Episode 50/200, Validation Accuracy: 1.0, Validation Loss: 0.016\n",
      "Epoc 7/40 Episode 100/200, Validation Accuracy: 0.8, Validation Loss: 0.066\n",
      "Epoc 7/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.149\n",
      "Epoch: 8\n",
      "Epoc 8/40 Episode 0/200, Validation Accuracy: 0.5, Validation Loss: 0.234\n",
      "Epoc 8/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.018\n",
      "Epoc 8/40 Episode 100/200, Validation Accuracy: 0.8, Validation Loss: 0.117\n",
      "Epoc 8/40 Episode 150/200, Validation Accuracy: 0.3, Validation Loss: 0.329\n",
      "Epoch: 9\n",
      "Epoc 9/40 Episode 0/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoc 9/40 Episode 50/200, Validation Accuracy: 0.3, Validation Loss: 0.307\n",
      "Epoc 9/40 Episode 100/200, Validation Accuracy: 0.8, Validation Loss: 0.153\n",
      "Epoc 9/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.226\n",
      "Epoch: 10\n",
      "Epoc 10/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.081\n",
      "Epoc 10/40 Episode 50/200, Validation Accuracy: 0.3, Validation Loss: 0.332\n",
      "Epoc 10/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.022\n",
      "Epoc 10/40 Episode 150/200, Validation Accuracy: 0.6, Validation Loss: 0.2\n",
      "Epoch: 11\n",
      "Epoc 11/40 Episode 0/200, Validation Accuracy: 1.0, Validation Loss: 0.016\n",
      "Epoc 11/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.052\n",
      "Epoc 11/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.21\n",
      "Epoc 11/40 Episode 150/200, Validation Accuracy: 0.8, Validation Loss: 0.148\n",
      "Epoch: 12\n",
      "Epoc 12/40 Episode 0/200, Validation Accuracy: 0.7, Validation Loss: 0.222\n",
      "Epoc 12/40 Episode 50/200, Validation Accuracy: 1.0, Validation Loss: 0.011\n",
      "Epoc 12/40 Episode 100/200, Validation Accuracy: 0.9, Validation Loss: 0.091\n",
      "Epoc 12/40 Episode 150/200, Validation Accuracy: 0.7, Validation Loss: 0.166\n",
      "Epoch: 13\n",
      "Epoc 13/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.102\n",
      "Epoc 13/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.093\n",
      "Epoc 13/40 Episode 100/200, Validation Accuracy: 0.6, Validation Loss: 0.242\n",
      "Epoc 13/40 Episode 150/200, Validation Accuracy: 0.8, Validation Loss: 0.197\n",
      "Epoch: 14\n",
      "Epoc 14/40 Episode 0/200, Validation Accuracy: 1.0, Validation Loss: 0.024\n",
      "Epoc 14/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.097\n",
      "Epoc 14/40 Episode 100/200, Validation Accuracy: 0.7, Validation Loss: 0.206\n",
      "Epoc 14/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.178\n",
      "Epoch: 15\n",
      "Epoc 15/40 Episode 0/200, Validation Accuracy: 0.3, Validation Loss: 0.293\n",
      "Epoc 15/40 Episode 50/200, Validation Accuracy: 0.8, Validation Loss: 0.182\n",
      "Epoc 15/40 Episode 100/200, Validation Accuracy: 0.9, Validation Loss: 0.228\n",
      "Epoc 15/40 Episode 150/200, Validation Accuracy: 0.9, Validation Loss: 0.059\n",
      "Epoch: 16\n",
      "Epoc 16/40 Episode 0/200, Validation Accuracy: 0.8, Validation Loss: 0.186\n",
      "Epoc 16/40 Episode 50/200, Validation Accuracy: 0.7, Validation Loss: 0.277\n",
      "Epoc 16/40 Episode 100/200, Validation Accuracy: 0.6, Validation Loss: 0.206\n",
      "Epoc 16/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.064\n",
      "Epoch: 17\n",
      "Epoc 17/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.069\n",
      "Epoc 17/40 Episode 50/200, Validation Accuracy: 0.3, Validation Loss: 0.307\n",
      "Epoc 17/40 Episode 100/200, Validation Accuracy: 0.8, Validation Loss: 0.197\n",
      "Epoc 17/40 Episode 150/200, Validation Accuracy: 0.5, Validation Loss: 0.315\n",
      "Epoch: 18\n",
      "Epoc 18/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.106\n",
      "Epoc 18/40 Episode 50/200, Validation Accuracy: 0.8, Validation Loss: 0.258\n",
      "Epoc 18/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.003\n",
      "Epoc 18/40 Episode 150/200, Validation Accuracy: 0.4, Validation Loss: 0.246\n",
      "Epoch: 19\n",
      "Epoc 19/40 Episode 0/200, Validation Accuracy: 0.8, Validation Loss: 0.171\n",
      "Epoc 19/40 Episode 50/200, Validation Accuracy: 0.6, Validation Loss: 0.299\n",
      "Epoc 19/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.004\n",
      "Epoc 19/40 Episode 150/200, Validation Accuracy: 0.8, Validation Loss: 0.085\n",
      "Epoch: 20\n",
      "Epoc 20/40 Episode 0/200, Validation Accuracy: 0.3, Validation Loss: 0.285\n",
      "Epoc 20/40 Episode 50/200, Validation Accuracy: 0.3, Validation Loss: 0.319\n",
      "Epoc 20/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.042\n",
      "Epoc 20/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.002\n",
      "Epoch: 21\n",
      "Epoc 21/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.195\n",
      "Epoc 21/40 Episode 50/200, Validation Accuracy: 0.5, Validation Loss: 0.355\n",
      "Epoc 21/40 Episode 100/200, Validation Accuracy: 0.7, Validation Loss: 0.206\n",
      "Epoc 21/40 Episode 150/200, Validation Accuracy: 0.6, Validation Loss: 0.32\n",
      "Epoch: 22\n",
      "Epoc 22/40 Episode 0/200, Validation Accuracy: 0.5, Validation Loss: 0.343\n",
      "Epoc 22/40 Episode 50/200, Validation Accuracy: 1.0, Validation Loss: 0.001\n",
      "Epoc 22/40 Episode 100/200, Validation Accuracy: 0.8, Validation Loss: 0.308\n",
      "Epoc 22/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.014\n",
      "Epoch: 23\n",
      "Epoc 23/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.03\n",
      "Epoc 23/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.1\n",
      "Epoc 23/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.179\n",
      "Epoc 23/40 Episode 150/200, Validation Accuracy: 0.8, Validation Loss: 0.183\n",
      "Epoch: 24\n",
      "Epoc 24/40 Episode 0/200, Validation Accuracy: 0.4, Validation Loss: 0.362\n",
      "Epoc 24/40 Episode 50/200, Validation Accuracy: 1.0, Validation Loss: 0.187\n",
      "Epoc 24/40 Episode 100/200, Validation Accuracy: 0.9, Validation Loss: 0.188\n",
      "Epoc 24/40 Episode 150/200, Validation Accuracy: 0.9, Validation Loss: 0.193\n",
      "Epoch: 25\n",
      "Epoc 25/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.024\n",
      "Epoc 25/40 Episode 50/200, Validation Accuracy: 1.0, Validation Loss: 0.001\n",
      "Epoc 25/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.001\n",
      "Epoc 25/40 Episode 150/200, Validation Accuracy: 0.9, Validation Loss: 0.247\n",
      "Epoch: 26\n",
      "Epoc 26/40 Episode 0/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoc 26/40 Episode 50/200, Validation Accuracy: 0.5, Validation Loss: 0.312\n",
      "Epoc 26/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoc 26/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.019\n",
      "Epoch: 27\n",
      "Epoc 27/40 Episode 0/200, Validation Accuracy: 0.7, Validation Loss: 0.171\n",
      "Epoc 27/40 Episode 50/200, Validation Accuracy: 1.0, Validation Loss: 0.16\n",
      "Epoc 27/40 Episode 100/200, Validation Accuracy: 0.7, Validation Loss: 0.143\n",
      "Epoc 27/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoch: 28\n",
      "Epoc 28/40 Episode 0/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoc 28/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.04\n",
      "Epoc 28/40 Episode 100/200, Validation Accuracy: 0.9, Validation Loss: 0.081\n",
      "Epoc 28/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.018\n",
      "Epoch: 29\n",
      "Epoc 29/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.2\n",
      "Epoc 29/40 Episode 50/200, Validation Accuracy: 0.8, Validation Loss: 0.081\n",
      "Epoc 29/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoc 29/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoch: 30\n",
      "Epoc 30/40 Episode 0/200, Validation Accuracy: 0.7, Validation Loss: 0.136\n",
      "Epoc 30/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.018\n",
      "Epoc 30/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.014\n",
      "Epoc 30/40 Episode 150/200, Validation Accuracy: 0.9, Validation Loss: 0.071\n",
      "Epoch: 31\n",
      "Epoc 31/40 Episode 0/200, Validation Accuracy: 1.0, Validation Loss: 0.02\n",
      "Epoc 31/40 Episode 50/200, Validation Accuracy: 0.4, Validation Loss: 0.339\n",
      "Epoc 31/40 Episode 100/200, Validation Accuracy: 0.9, Validation Loss: 0.118\n",
      "Epoc 31/40 Episode 150/200, Validation Accuracy: 0.9, Validation Loss: 0.083\n",
      "Epoch: 32\n",
      "Epoc 32/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.036\n",
      "Epoc 32/40 Episode 50/200, Validation Accuracy: 1.0, Validation Loss: 0.001\n",
      "Epoc 32/40 Episode 100/200, Validation Accuracy: 0.8, Validation Loss: 0.202\n",
      "Epoc 32/40 Episode 150/200, Validation Accuracy: 0.9, Validation Loss: 0.141\n",
      "Epoch: 33\n",
      "Epoc 33/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.077\n",
      "Epoc 33/40 Episode 50/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoc 33/40 Episode 100/200, Validation Accuracy: 0.5, Validation Loss: 0.375\n",
      "Epoc 33/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.061\n",
      "Epoch: 34\n",
      "Epoc 34/40 Episode 0/200, Validation Accuracy: 0.3, Validation Loss: 0.33\n",
      "Epoc 34/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.038\n",
      "Epoc 34/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoc 34/40 Episode 150/200, Validation Accuracy: 0.9, Validation Loss: 0.036\n",
      "Epoch: 35\n",
      "Epoc 35/40 Episode 0/200, Validation Accuracy: 0.5, Validation Loss: 0.277\n",
      "Epoc 35/40 Episode 50/200, Validation Accuracy: 0.8, Validation Loss: 0.236\n",
      "Epoc 35/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.003\n",
      "Epoc 35/40 Episode 150/200, Validation Accuracy: 0.3, Validation Loss: 0.317\n",
      "Epoch: 36\n",
      "Epoc 36/40 Episode 0/200, Validation Accuracy: 1.0, Validation Loss: 0.167\n",
      "Epoc 36/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.178\n",
      "Epoc 36/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.197\n",
      "Epoc 36/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.001\n",
      "Epoch: 37\n",
      "Epoc 37/40 Episode 0/200, Validation Accuracy: 0.9, Validation Loss: 0.125\n",
      "Epoc 37/40 Episode 50/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoc 37/40 Episode 100/200, Validation Accuracy: 0.3, Validation Loss: 0.36\n",
      "Epoc 37/40 Episode 150/200, Validation Accuracy: 0.4, Validation Loss: 0.309\n",
      "Epoch: 38\n",
      "Epoc 38/40 Episode 0/200, Validation Accuracy: 0.6, Validation Loss: 0.412\n",
      "Epoc 38/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.116\n",
      "Epoc 38/40 Episode 100/200, Validation Accuracy: 1.0, Validation Loss: 0.0\n",
      "Epoc 38/40 Episode 150/200, Validation Accuracy: 1.0, Validation Loss: 0.007\n",
      "Epoch: 39\n",
      "Epoc 39/40 Episode 0/200, Validation Accuracy: 0.3, Validation Loss: 0.293\n",
      "Epoc 39/40 Episode 50/200, Validation Accuracy: 0.9, Validation Loss: 0.119\n",
      "Epoc 39/40 Episode 100/200, Validation Accuracy: 0.5, Validation Loss: 0.229\n",
      "Epoc 39/40 Episode 150/200, Validation Accuracy: 0.8, Validation Loss: 0.149\n",
      "Testing ... . . . .. . . . \n",
      "Meta test Episode 49/1000, Test Accuracy: 0.6, Test Loss: 0.17\n",
      "Meta test Episode 99/1000, Test Accuracy: 1.0, Test Loss: 0.002\n",
      "Meta test Episode 149/1000, Test Accuracy: 0.4, Test Loss: 0.312\n",
      "Meta test Episode 199/1000, Test Accuracy: 0.8, Test Loss: 0.158\n",
      "Meta test Episode 249/1000, Test Accuracy: 1.0, Test Loss: 0.005\n",
      "Meta test Episode 299/1000, Test Accuracy: 1.0, Test Loss: 0.0\n",
      "Meta test Episode 349/1000, Test Accuracy: 1.0, Test Loss: 0.0\n",
      "Meta test Episode 399/1000, Test Accuracy: 0.9, Test Loss: 0.172\n",
      "Meta test Episode 449/1000, Test Accuracy: 1.0, Test Loss: 0.0\n",
      "Meta test Episode 499/1000, Test Accuracy: 0.8, Test Loss: 0.305\n",
      "Meta test Episode 549/1000, Test Accuracy: 0.6, Test Loss: 0.161\n",
      "Meta test Episode 599/1000, Test Accuracy: 0.9, Test Loss: 0.095\n",
      "Meta test Episode 649/1000, Test Accuracy: 0.4, Test Loss: 0.258\n",
      "Meta test Episode 699/1000, Test Accuracy: 0.5, Test Loss: 0.269\n",
      "Meta test Episode 749/1000, Test Accuracy: 0.3, Test Loss: 0.279\n",
      "Meta test Episode 799/1000, Test Accuracy: 1.0, Test Loss: 0.057\n",
      "Meta test Episode 849/1000, Test Accuracy: 1.0, Test Loss: 0.001\n",
      "Meta test Episode 899/1000, Test Accuracy: 1.0, Test Loss: 0.0\n",
      "Meta test Episode 949/1000, Test Accuracy: 1.0, Test Loss: 0.009\n",
      "Meta test Episode 999/1000, Test Accuracy: 1.0, Test Loss: 0.029\n"
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    print(f'Epoch: {ep}')\n",
    "    for epi in range(num_episodes):\n",
    "        x, y = text_generator_.sample_batch('meta_train', 1, shuffle = True)\n",
    "        support_input_t, query_input_t, labels_onehot = get_latents_new(x,y, embed_size, n_way, n_query, k_shot)\n",
    "        x_latent = model_text(support_input_t)\n",
    "        q_latent = model_text(query_input_t)\n",
    "        # Compute and print loss\n",
    "        loss, accuracy = criterion(x_latent, q_latent, torch.tensor(labels_onehot))\n",
    "        optimizer_text.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_text.step()\n",
    "        if epi % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                valid_x, valid_y = text_generator_.sample_batch('meta_val', 1, shuffle = True)\n",
    "                support_input_valid, query_input_valid, labels_onehot_valid = get_latents_new(valid_x,valid_y, embed_size, n_way, n_query, k_shot)\n",
    "                x_latent_valid = model_text(support_input_valid)\n",
    "                q_latent_valid = model_text(query_input_valid)\n",
    "                # Compute and print loss\n",
    "                valid_loss, valid_acc = criterion(x_latent_valid, q_latent_valid, torch.tensor(labels_onehot_valid))\n",
    "                print(f'Epoc {ep}/{num_epochs} Episode {epi}/{num_episodes}, Validation Accuracy: {round(valid_acc.item(),3)}, Validation Loss: {round(valid_loss.item(),3)}')\n",
    "print('Testing ... . . . .. . . . ')\n",
    "meta_test_accuracies = []\n",
    "for epi in range(1000):\n",
    "    test_x, test_y = text_generator_.sample_batch('meta_test', 1, shuffle = True)\n",
    "    support_input_test, query_input_test, labels_onehot_test = get_latents_new(test_x,test_y, embed_size, n_way, n_query, k_shot)\n",
    "    with torch.no_grad():\n",
    "        x_latent_test = model_text(support_input_test)\n",
    "        q_latent_test = model_text(query_input_test)\n",
    "        # Compute and print loss\n",
    "        test_loss, test_acc = criterion(x_latent_test, q_latent_test, torch.tensor(labels_onehot_valid))\n",
    "        if (epi + 1) % 50 == 0:\n",
    "            print(f'Meta test Episode {epi}/{1000}, Test Accuracy: {round(test_acc.item(),3)}, Test Loss: {round(test_loss.item(),3)}')\n",
    "        meta_test_accuracies.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Meta-Test Accuracy: 0.83860, Meta-Test Accuracy Std: 0.22740\n"
     ]
    }
   ],
   "source": [
    "avg_acc = np.mean(meta_test_accuracies)\n",
    "stds = np.std(meta_test_accuracies)\n",
    "print('Average Meta-Test Accuracy: {:.5f}, Meta-Test Accuracy Std: {:.5f}'.format(avg_acc, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
